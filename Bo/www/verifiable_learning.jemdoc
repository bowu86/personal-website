# jemdoc: menu{MENU}{verifiable_learning.html}, title{Bo Wu - Research},nodefaultcss,addcss{jemdoc.css}

== Verifiable Reinforcement Learning
~~~
Achieving higher levels of autonomy in uncertain, unstructured, and dynamic environments increasingly relies on state-of-art machine learning techniques, especially reinforcement learning (RL) for automated sequential decision-making. However, several recent high-profile deadly accidents that involve semi-autonomous vehicles have raised concerns about whether current learning-based methods can ever lead us to future full autonomy. 

On the other hand, approaches rooted in formal methods for verification and synthesis can provide guarantees but have difficulty in efficiently reasoning about uncertainty and the correctness of data-driven models. My research combines these two seemingly incompatible paradigms, machine learning and formal methods, to effectively design reliable autonomous systems.
~~~

=== *JIRP: Joint Inference of Reward Machines and Policies for Reinforcement Learning*



=== *Robust learning-based human-robot collaboration with performance guarantees*
~~~
{}{img_left}{pic\baxter.jpg}{}{1000}{300}
~~~

~~~
{}{img_left}{pic\hri.png}{}{350}{400}
A reliable and effective collaboration between human and machine has been recognized as being important or even necessary for many safety-critical systems, such as intelligent transportation and manufacturing automation. Achieving such a reliable and effective human-machine collaboration, however, is fairly challenging so far due to the complexities introduced by various types of system uncertainties that may arise from human operation, machine dynamics as well as dynamically changing environments and dynamic interactions between human.

This project it to build a unified formal design framework enabling an efficient human-machine collaboration with performance guarantee in linear temporal logic (LTL) formulas. The ideas are drawn from control theory, machine learning and human factor engineering.  I developed probably-approximately-correct (PAC) learning algorithm to find the optimal policy subject to safety specifications and complex temporal goals in a data-efficient manner. The difference between the learned optimal policy and the true optimal policy is proven to be bounded by an arbitrarily small constant with a high confidence level.


Publication:
. [https://arxiv.org/abs/1706.00007 A Learning Based Human Robot Collaboration with Temporal Logic Constraints] \n 
  *B. Wu*, B. Hu, and H. Lin. /IEEE Transactions on Automation Science and Engineering/, under revision, 2018.
. Toward Efficient Manufaturing Systems: a Trust Based Human Robot collaboration \n
  *B. Wu*, B. Hu, and H. Lin. /American Control Conference (ACC)/, pp. 1536-1541, 2017
~~~



