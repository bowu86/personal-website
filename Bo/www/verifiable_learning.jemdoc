# jemdoc: menu{MENU}{verifiable_learning.html}, title{Bo Wu - Research},nodefaultcss,addcss{jemdoc.css}

= Verifiable Reinforcement Learning
~~~
{}{img_left}{pic\rl.png}{}{450}{300}
- Achieving higher levels of autonomy in uncertain, unstructured, and dynamic environments increasingly relies on state-of-art machine learning techniques, especially reinforcement learning (RL) for automated sequential decision-making. 

- However, several recent high-profile deadly accidents that involve semi-autonomous vehicles have raised concerns about whether current learning-based methods can ever lead us to future full autonomy. 

- On the other hand, approaches rooted in formal methods for verification and synthesis can provide guarantees but have difficulty in efficiently reasoning about uncertainty and the correctness of data-driven models. My research combines these two seemingly incompatible paradigms, machine learning and formal methods, to effectively design reliable autonomous systems.
~~~

== *JIRP: Joint Inference of Reward Machines and Policies for Reinforcement Learning*
~~~
{}{img_left}{pic\jirp.png}{}{500}{300}
- A bottleneck that limits the wide adoption of RL-based autonomous systems is that existing RL methods are very data-hungry and lack transparency to explain their decisions. 

- To this end, we employ a new hierarchical RL (HRL) framework, which integrates symbolic inference that results in compact symbolic representations to effectively boost and interpret the learning process. The inferred symbolic representation helps explain why and how an agent gets rewarded in a sparse reward environment. 

- We apply two different automaton inferences methods: SAT and RPNI (Regular Positive Negative Inference) to learn the high-level representation which is used to boost RL. 

- The proposed algorithm is proven to converge almost surely to an optimal policy and is shown to outperform state-of-art RL methods in extensive experiments.
 
~~~

Publication:

. [https://arxiv.org/abs/1909.05912 Joint Inference of Reward Machines and Policies for Reinforcement Learning]\n
  Z. Xu, I. Gavran, Y. Ahmad, R. Majumdar, D. Neider, U. Topcu, and *B. Wu*. The first two authors have contributed equally; the rest of the authors are ordered alphabetically. To be submitted to /The International Conference on Automated Planning and Scheduling (ICAPS)/, 2020.

== *Robust learning-based human-robot collaboration with performance guarantees*
~~~
{}{img_left}{pic\baxter.jpg}{}{1000}{300}
~~~

~~~
{}{img_left}{pic\hri.png}{}{350}{400}
A reliable and effective collaboration between human and machine has been recognized as being important or even necessary for many safety-critical systems, such as intelligent transportation and manufacturing automation. Achieving such a reliable and effective human-machine collaboration, however, is fairly challenging so far due to the complexities introduced by various types of system uncertainties that may arise from human operation, machine dynamics as well as dynamically changing environments and dynamic interactions between human.

This project it to build a unified formal design framework enabling an efficient human-machine collaboration with performance guarantee in linear temporal logic (LTL) formulas. The ideas are drawn from control theory, machine learning and human factor engineering.  I developed probably-approximately-correct (PAC) learning algorithm to find the optimal policy subject to safety specifications and complex temporal goals in a data-efficient manner. The difference between the learned optimal policy and the true optimal policy is proven to be bounded by an arbitrarily small constant with a high confidence level.


Publication:
. [https://arxiv.org/abs/1706.00007 A Learning Based Human Robot Collaboration with Temporal Logic Constraints] \n 
  *B. Wu*, B. Hu, and H. Lin. /IEEE Transactions on Automation Science and Engineering/, under revision, 2018.
. Toward Efficient Manufaturing Systems: a Trust Based Human Robot collaboration \n
  *B. Wu*, B. Hu, and H. Lin. /American Control Conference (ACC)/, pp. 1536-1541, 2017
~~~



